{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import time\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import schema\n",
    "import data\n",
    "import cerberus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('torrance.xml')\n",
    "sqlite_file = 'torrance.xml'\n",
    "root = tree.getroot()\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Project - Torrance, CA\n",
    "\n",
    "At first, I used a map of Los Angeles, CA, which is where I grew up. Unfortunately, with the size of the file, it was tough to sort through the data manually (if I ever wanted to check for myself). As a result, I shortened the data by just drawing a map of the neighborhood where I spent most of my childhood - Torrance, CA.\n",
    "\n",
    "[https://mapzen.com/data/metro-extracts/your-extracts/eb4ca3779a47](https://mapzen.com/data/metro-extracts/your-extracts/eb4ca3779a47 \"My hometown!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 5950,\n",
      " 'nd': 1701210,\n",
      " 'node': 1540164,\n",
      " 'osm': 1,\n",
      " 'relation': 1652,\n",
      " 'tag': 997540,\n",
      " 'way': 150132}\n"
     ]
    }
   ],
   "source": [
    "def count_tags(filename):\n",
    "    tagsDict = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tagsDict.keys():\n",
    "            tagsDict[elem.tag]+=1\n",
    "        else:\n",
    "            tagsDict[elem.tag]=1\n",
    "    return tagsDict\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('torrance.xml')\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "To begin the audit of the data file, let's get some general information first. I wanted to see the frequency of different tags in the dataset, which can help us imagine a picture of the area before we even begin. For example, we see that there are 150,132 *ways*, which represent different streets on the map. The 1,540,164 *nodes* represent defining points in space on the map. For a map of just a neighborhood, we can see that there are massive amounts of information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "default = defaultdict(set)\n",
    "expected = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arno': {'Plaza del Arno'},\n",
      " 'Ave': {'Normandie Ave', 'Pier Ave', 'Inglewood Ave'},\n",
      " 'Ave.': {'Ocean Ave.', 'South Western Ave.'},\n",
      " 'Blvd': {'Crenshaw Blvd',\n",
      "          'Hawthorne Blvd',\n",
      "          'Torrance Blvd',\n",
      "          'West Redondo Beach Blvd'},\n",
      " 'Blvd.': {'Palos Verdes Blvd.', 'West Redondo Beach Blvd.'},\n",
      " 'Ctr.': {'Peninsula Ctr.'},\n",
      " 'East': {'Palos Verdes Drive East'},\n",
      " 'Highway': {'E Pacific Coast Highway',\n",
      "             'Pacific Coast Highway',\n",
      "             'West Pacific Coast Highway'},\n",
      " 'Monte': {'Via del Monte'},\n",
      " 'Ness': {'Van Ness'},\n",
      " 'St': {'Carson St'},\n",
      " 'Torrance': {'Pacific Coast Highway Torrance'},\n",
      " 'street': {'W. 190th street'}}\n"
     ]
    }
   ],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(set)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Way\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    for event, elem, in ET.iterparse('torrance.xml', events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    pprint.pprint(dict(street_types))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "One of the first audits was to check for any irregularities with how the streets are named. We are all used to (based on where you are in this world) the typical set of names for a street:\n",
    "\n",
    "- Street\n",
    "- Avenue\n",
    "- Boulevard\n",
    "- Drive\n",
    "- Court\n",
    "- Place\n",
    "- Way\n",
    "- Square\n",
    "- Lane\n",
    "- Road\n",
    "- Trail\n",
    "- Parkway\n",
    "- Commons\n",
    "\n",
    "I wanted to check the value of all **way** tags against the list above, and only see any street names that did not match. Upon my results, I found that there were a few discrepancies found. As the results above show, some abbreviations had a period at the end, while others didn't. Unique street endings, whether it was a direction (East) or no designation of the type of street (\"Van Ness\"), also made the list. When I compared my *northtorrance.xml* file before my *torrance.xml* file, I noticed that there were no errors found on that file. Interesting to see how a smaller neighborhood can have no errors, while the bigger city still does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "zipcode_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "zipcode = defaultdict(set)\n",
    "\n",
    "expecteds = [\"90248\", \"90278\", \"90501\", \"90502\", \"90503\", \"90504\", \"90505\", \"90506\", \"90507\", \"90508\", \"90509\", \n",
    "            \"90510\", \"90717\", \"90277\"]\n",
    "\n",
    "def audit_zip(zipcode, zip):\n",
    "    m = zipcode_re.search(zip)\n",
    "    if m:\n",
    "        zcode = m.group()\n",
    "        if zcode not in expected:\n",
    "            zipcode[zcode].add(zip)\n",
    "\n",
    "def is_zip(elem):\n",
    "    return (elem.attrib['k'] == \"postcode\")\n",
    "\n",
    "def zc():\n",
    "    for event, elem, in ET.iterparse('torrance.xml', events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip(tag):\n",
    "                    audit_zip(zipcode, tag.attrib['v'])\n",
    "    pprint.pprint(dict(zipcode))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    zc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that no issues with street names were found, I was not confident to deem the dataset clean. I ran the same check with zip codes, to check if all zip codes reported in the dataset are correct. The only discrepancy that I found was the zip code for a neighboring city: Lawndale. By searching for that value in the dataset, I noticed that the value was stored against the keys *zip_left* and *zip_right*, which lead me to believe that the information was used when describing certain regions closer to the border of the Torrance neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Additional Info\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'christian': {'christian'}}\n"
     ]
    }
   ],
   "source": [
    "religion_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "religious = defaultdict(set)\n",
    "\n",
    "expect = []\n",
    "\n",
    "def audit_rel(religious, rel):\n",
    "    m = religion_re.search(rel)\n",
    "    if m:\n",
    "        rcode = m.group()\n",
    "        if rcode not in expected:\n",
    "            religious[rcode].add(rel)\n",
    "\n",
    "def is_rel(elem):\n",
    "    return (elem.attrib['k'] == \"religion\")\n",
    "\n",
    "def rc():\n",
    "    for event, elem, in ET.iterparse('torrance.xml', events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_rel(tag):\n",
    "                    audit_rel(religious, tag.attrib['v'])\n",
    "    pprint.pprint(dict(religious))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    rc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to use the same structure to help investigate some additional info about my hometown. Here, we can see that the places of worship around Torrance are all Christian churches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'atm': {'atm'},\n",
      " 'bank': {'bank'},\n",
      " 'bar': {'bar'},\n",
      " 'cafe': {'cafe'},\n",
      " 'car_rental': {'car_rental'},\n",
      " 'childcare': {'childcare'},\n",
      " 'cinema': {'cinema'},\n",
      " 'clinic': {'clinic'},\n",
      " 'college': {'college'},\n",
      " 'community_centre': {'community_centre'},\n",
      " 'doctors': {'doctors'},\n",
      " 'fast_food': {'fast_food'},\n",
      " 'fire_station': {'fire_station'},\n",
      " 'fuel': {'fuel'},\n",
      " 'grave_yard': {'grave_yard'},\n",
      " 'hospital': {'hospital'},\n",
      " 'kindergarten': {'kindergarten'},\n",
      " 'language_school': {'language_school'},\n",
      " 'library': {'library'},\n",
      " 'marketplace': {'marketplace'},\n",
      " 'parking': {'parking'},\n",
      " 'parking_space': {'parking_space'},\n",
      " 'pharmacy': {'pharmacy'},\n",
      " 'place_of_worship': {'place_of_worship'},\n",
      " 'police': {'police'},\n",
      " 'post_office': {'post_office'},\n",
      " 'restaurant': {'restaurant'},\n",
      " 'school': {'school'},\n",
      " 'shelter': {'shelter'},\n",
      " 'social_centre': {'social_centre'},\n",
      " 'social_facility': {'social_facility'},\n",
      " 'spa': {'spa'},\n",
      " 'swimming_pool': {'swimming_pool'},\n",
      " 'theatre': {'theatre'},\n",
      " 'toilets': {'toilets'},\n",
      " 'veterinary': {'veterinary'},\n",
      " 'whirlpool': {'whirlpool'}}\n"
     ]
    }
   ],
   "source": [
    "def audit_buildings(default, x):\n",
    "    m = comp.search(x)\n",
    "    if m:\n",
    "        zcode = m.group()\n",
    "        if zcode not in expected:\n",
    "            default[zcode].add(x)\n",
    "\n",
    "def is_place(elem):\n",
    "    return (elem.attrib['k'] == \"amenity\")\n",
    "\n",
    "def audit_b():\n",
    "    for event, elem, in ET.iterparse('torrance.xml', events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_place(tag):\n",
    "                    audit_buildings(default, tag.attrib['v'])\n",
    "    pprint.pprint(dict(default))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    audit_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's pretty funny to see that they marked toilets on this map. Also, there's a whirlpool?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After some analysis with the Torrance data, there is some room for improvement. The street names are affected by user imput - some users left a period after abbreviating, others didn't. Postal codes, on the other hand, seem to be accurate. This shows that the user input was precise, and indicates that the current version is accurate as is.\n",
    "\n",
    "### Additional Thoughts\n",
    "\n",
    "The extraction of the map data from the website was rather difficult. The instructions provided seemed to be a bit outdated, and it required a couple of extra steps than what was provided. It made more sense after I ordered the data, but I could see how it could be overwhelming to someone who is experiencing the website for the first time.\n",
    "\n",
    "With that being said, I would like to suggest a tutorial, or tour, of the site and how to pull the data.\n",
    "\n",
    "A benefit can result in more web traffic. Saving keystrokes can improve user experience and increase user retention. With the increase in users, the data can be updated more frequently. The site can also decide to connect the users as well.\n",
    "\n",
    "The downside to the tutorial would affect those that are familiar with how the extraction of data works. It would provide unneccesary barriers of entry, which can deter users away. I would also anticipate users not feeling that the tutorial is of any help, which would affect all visitors to the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store...............................: 6K   \n",
      "DAND P4 - OpenStreetMap Project.ipynb...: 16K  \n",
      "data.py.................................: 10K  \n",
      "nodes.csv...............................: 0B   \n",
      "nodes_tags.csv..........................: 0B   \n",
      "northtorrance.xml.......................: 41M  \n",
      "schema.py...............................: 2K   \n",
      "torrance.xml............................: 337M \n",
      "ways.csv................................: 0B   \n",
      "ways_nodes.csv..........................: 0B   \n",
      "ways_tags.csv...........................: 0B   \n",
      "DAND P4 - OpenStreetMap Project-checkpoint.ipynb: 14K  \n",
      "DAND P4 - OpenStreetMap-checkpoint.ipynb: 1K   \n",
      "OpenStreetMap DAND P3-checkpoint.ipynb..: 72B  \n",
      "osm.py-checkpoint.ipynb.................: 72B  \n",
      "data.cpython-35.pyc.....................: 10K  \n",
      "schema.cpython-35.pyc...................: 1K   \n"
     ]
    }
   ],
   "source": [
    "from hurry.filesize import size\n",
    "import os\n",
    "\n",
    "dirpath = '/Users/kangsankim/Desktop/Projects/UdacityDAND/DANDP4'\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "\n",
    "for filename, size in files_list:\n",
    "    print ('{:.<40s}: {:5s}'.format(filename,size))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
